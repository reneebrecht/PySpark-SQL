{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d31362fa",
   "metadata": {},
   "source": [
    "# PySpark and Big Data Project -  Solutions\n",
    "\n",
    "\n",
    "The Common Crawl is a non-profit organization that crawls, archives, and analyzes content on all public websites. The Common Crawl maintains petabytes (thousands of terabytes!) of web content and insights derived from their analyses, all of which is made publicly available for research and educational purposes.\n",
    "\n",
    "In this project, I will be working with a small portion of a dataset published by the Common Crawl. Analyzing Common Crawl data can be easier if you know a few key facts about the format of domain names. Every website’s name is composed of multiple parts, as illustrated in the following diagram:\n",
    "\n",
    "![Local Image](final-project-url.svg)\n",
    "\n",
    "\n",
    "The dataset we’ll be working with, the domain graph, contains a record of every domain on the internet and the count of subdomains associated with the site."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f30bf",
   "metadata": {},
   "source": [
    "## Task Group 1 - Analyzing Common Crawl Data with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c96e6",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Initialize a new Spark Context and read in the domain graph as an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d3ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba7d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['367855\\t172-in-addr\\tarpa\\t1',\n",
       " '367856\\taddr\\tarpa\\t1',\n",
       " '367857\\tamphic\\tarpa\\t1',\n",
       " '367858\\tbeta\\tarpa\\t1',\n",
       " '367859\\tcallic\\tarpa\\t1',\n",
       " '367860\\tch\\tarpa\\t1',\n",
       " '367861\\td\\tarpa\\t1',\n",
       " '367862\\thome\\tarpa\\t7',\n",
       " '367863\\tiana\\tarpa\\t1',\n",
       " '367907\\tlocal\\tarpa\\t1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Domains CSV File into an RDD\n",
    "common_crawl_domain_counts = sc.textFile('./crawl/cc-main-limited-domains.csv')\n",
    "\n",
    "# Display first few domains from the RDD\n",
    "common_crawl_domain_counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f18a601",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Apply `fmt_domain_graph_entry` over `common_crawl_domain_counts` and save the result as a new RDD named `formatted_host_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7950d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_domain_graph_entry(entry):\n",
    "    \"\"\"\n",
    "    Formats a Common Crawl domain graph entry. Extracts the site_id, \n",
    "    top-level domain (tld), domain name, and subdomain count as seperate items.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the entry on delimiter ('\\t') into site_id, domain, tld, and num_subdomains\n",
    "    site_id, domain, tld, num_subdomains = entry.split('\\t')        \n",
    "    return int(site_id), domain, tld, int(num_subdomains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9ed9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(367855, '172-in-addr', 'arpa', 1),\n",
       " (367856, 'addr', 'arpa', 1),\n",
       " (367857, 'amphic', 'arpa', 1),\n",
       " (367858, 'beta', 'arpa', 1),\n",
       " (367859, 'callic', 'arpa', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply `fmt_domain_graph_entry` to the raw data RDD\n",
    "formatted_host_counts = common_crawl_domain_counts.map(lambda x: fmt_domain_graph_entry(x))\n",
    "\n",
    "# Display the first few entries of the new RDD\n",
    "formatted_host_counts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a9aae",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Apply `extract_subdomain_counts` over `common_crawl_domain_counts` and save the result as a new RDD named `host_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb75dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_subdomain_counts(entry):\n",
    "    \"\"\"\n",
    "    Extract the subdomain count from a Common Crawl domain graph entry.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the entry on delimiter ('\\t') into site_id, domain, tld, and num_subdomains\n",
    "    site_id, domain, tld, num_subdomains = entry.split('\\t')\n",
    "    \n",
    "    # return ONLY the num_subdomains\n",
    "    return int(num_subdomains)\n",
    "\n",
    "\n",
    "# Apply `extract_subdomain_counts` to the raw data RDD\n",
    "host_counts =  common_crawl_domain_counts.map(lambda x: extract_subdomain_counts(x))\n",
    "\n",
    "# Display the first few entries\n",
    "host_counts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44483f3",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Using `host_counts`, calculate the total number of subdomains across all domains in the dataset, save the result to a variable named `total_host_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa284001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595466"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce the RDD to a single value, the sum of subdomains, with a lambda function\n",
    "# as the reduce function\n",
    "total_host_counts = host_counts.reduce(lambda a, b: a + b)\n",
    "\n",
    "# Display result count\n",
    "total_host_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f5579e",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "Stop the current `SparkSession` and `sparkContext` before moving on to analyze the data with SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562745d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the sparkContext and the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129687d",
   "metadata": {},
   "source": [
    "## Task Group 2 - Exploring Domain Counts with PySpark DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22e2b6",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "Create a new `SparkSession` and assign it to a variable named `spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99565365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f3a0c",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "Read `./crawl/cc-main-limited-domains.csv` into a new Spark DataFrame named `common_crawl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297084db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----+---+\n",
      "|   _c0|        _c1| _c2|_c3|\n",
      "+------+-----------+----+---+\n",
      "|367855|172-in-addr|arpa|  1|\n",
      "|367856|       addr|arpa|  1|\n",
      "|367857|     amphic|arpa|  1|\n",
      "|367858|       beta|arpa|  1|\n",
      "|367859|     callic|arpa|  1|\n",
      "+------+-----------+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the target file into a DataFrame\n",
    "common_crawl = spark.read.option('delimiter', '\\t').option('inferSchema', True).csv('./crawl/cc-main-limited-domains.csv') \n",
    "\n",
    "\n",
    "# Display the DataFrame to the notebook\n",
    "common_crawl.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23082fd2",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "Rename the DataFrame's columns to the following: \n",
    "\n",
    "- site_id\n",
    "- domain\n",
    "- top_level_domain\n",
    "- num_subdomains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7b4ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------------+--------------+\n",
      "|site_id|domain     |top_level_domain|num_subdomains|\n",
      "+-------+-----------+----------------+--------------+\n",
      "|367855 |172-in-addr|arpa            |1             |\n",
      "|367856 |addr       |arpa            |1             |\n",
      "|367857 |amphic     |arpa            |1             |\n",
      "|367858 |beta       |arpa            |1             |\n",
      "|367859 |callic     |arpa            |1             |\n",
      "+-------+-----------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- site_id: integer (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- top_level_domain: string (nullable = true)\n",
      " |-- num_subdomains: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the DataFrame's columns with `withColumnRenamed()`\n",
    "common_crawl = common_crawl\\\n",
    "    .withColumnRenamed(\"_c0\", \"site_id\")\\\n",
    "    .withColumnRenamed(\"_c1\", \"domain\")\\\n",
    "    .withColumnRenamed(\"_c2\", \"top_level_domain\")\\\n",
    "    .withColumnRenamed(\"_c3\", \"num_subdomains\")\n",
    "\n",
    "\n",
    "  \n",
    "# Display the first few rows of the DataFrame and the new schema\n",
    "common_crawl.show(5, truncate=False)\n",
    "common_crawl.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff524e08",
   "metadata": {},
   "source": [
    "## Task Group 3 - Reading and Writing Datasets to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00bc518",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "Save the `common_crawl` DataFrame as parquet files in a directory called `./results/common_crawl/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the `common_crawl` DataFrame to a series of parquet files\n",
    "\n",
    "common_crawl.write.parquet('./results/common_crawl/', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbf8ff",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "\n",
    "Read `./results/common_crawl/` into a new DataFrame to confirm our DataFrame was saved properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ceb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------------+--------------+\n",
      "|site_id|domain     |top_level_domain|num_subdomains|\n",
      "+-------+-----------+----------------+--------------+\n",
      "|367855 |172-in-addr|arpa            |1             |\n",
      "|367856 |addr       |arpa            |1             |\n",
      "|367857 |amphic     |arpa            |1             |\n",
      "|367858 |beta       |arpa            |1             |\n",
      "|367859 |callic     |arpa            |1             |\n",
      "+-------+-----------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- site_id: integer (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- top_level_domain: string (nullable = true)\n",
      " |-- num_subdomains: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from parquet directory\n",
    "common_crawl_domains = spark.read.parquet('./results/common_crawl/') \n",
    "\n",
    "# Display the first few rows of the DataFrame and the schema\n",
    "common_crawl.show(5, truncate=False)\n",
    "common_crawl.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f34ede",
   "metadata": {},
   "source": [
    "## Task Group 4 - Querying Domain Counts with PySpark DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71895cf",
   "metadata": {},
   "source": [
    "### Task 11\n",
    "\n",
    "Create a local temporary view from `common_crawl_domains`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd04b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view in the metadata for this `SparkSession`\n",
    "common_crawl_domains.createOrReplaceTempView('crawl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c2ef4f",
   "metadata": {},
   "source": [
    "### Task 12\n",
    "\n",
    "Calculate the total number of domains for each top-level domain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f79679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|top_level_domain|count|\n",
      "+----------------+-----+\n",
      "|edu             |18547|\n",
      "|gov             |15007|\n",
      "|travel          |6313 |\n",
      "|coop            |5319 |\n",
      "|jobs            |3893 |\n",
      "|post            |117  |\n",
      "|map             |34   |\n",
      "|arpa            |11   |\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using DataFrame methods\n",
    "common_crawl_domains.groupby('top_level_domain').count().orderBy('count', ascending=False).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|top_level_domain|count|\n",
      "+----------------+-----+\n",
      "|edu             |18547|\n",
      "|gov             |15007|\n",
      "|travel          |6313 |\n",
      "|coop            |5319 |\n",
      "|jobs            |3893 |\n",
      "|post            |117  |\n",
      "|map             |34   |\n",
      "|arpa            |11   |\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using SQL\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        top_level_domain, \n",
    "        COUNT(domain) AS count\n",
    "    FROM crawl\n",
    "    GROUP BY top_level_domain\n",
    "    ORDER BY COUNT(domain) DESC\n",
    "    \"\"\"\n",
    ").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb11c33",
   "metadata": {},
   "source": [
    "### Task 13\n",
    "\n",
    "Calculate the total number of subdomains for each top-level domain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502578e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|top_level_domain|sum(num_subdomains)|\n",
      "+----------------+-------------------+\n",
      "|edu             |484438             |\n",
      "|gov             |85354              |\n",
      "|travel          |10768              |\n",
      "|coop            |8683               |\n",
      "|jobs            |6023               |\n",
      "|post            |143                |\n",
      "|map             |40                 |\n",
      "|arpa            |17                 |\n",
      "+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using DataFrame methods\n",
    "\n",
    "common_crawl_domains\\\n",
    "    .groupby('top_level_domain')\\\n",
    "    .sum('num_subdomains')\\\n",
    "    .orderBy('sum(num_subdomains)', ascending=False)\\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4539ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+\n",
      "|top_level_domain|total_count|\n",
      "+----------------+-----------+\n",
      "|edu             |484438     |\n",
      "|gov             |85354      |\n",
      "|travel          |10768      |\n",
      "|coop            |8683       |\n",
      "|jobs            |6023       |\n",
      "|post            |143        |\n",
      "|map             |40         |\n",
      "|arpa            |17         |\n",
      "+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using SQL\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        top_level_domain, \n",
    "        SUM(num_subdomains) AS total_count\n",
    "    FROM crawl\n",
    "    GROUP BY top_level_domain\n",
    "    ORDER BY SUM(num_subdomains) DESC\n",
    "    \"\"\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2dfea3",
   "metadata": {},
   "source": [
    "### Task 14\n",
    "\n",
    "How many sub-domains does `nps.gov` have? Filter the dataset to that website's entry, display the columns `top_level_domain`, `domain`, and `num_subdomains` in your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45051e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+--------------+\n",
      "|top_level_domain|domain|num_subdomains|\n",
      "+----------------+------+--------------+\n",
      "|gov             |nps   |178           |\n",
      "+----------------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame using DataFrame Methods\n",
    "common_crawl_domains\\\n",
    "    .select(['top_level_domain', 'domain', 'num_subdomains'])\\\n",
    "    .filter(common_crawl_domains.domain == \"nps\")\\\n",
    "    .filter(common_crawl_domains.top_level_domain == \"gov\")\\\n",
    "    .show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1746a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+--------------+\n",
      "|top_level_domain|domain|num_subdomains|\n",
      "+----------------+------+--------------+\n",
      "|gov             |nps   |178           |\n",
      "+----------------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame using SQL\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT top_level_domain, domain, num_subdomains\n",
    "    FROM crawl\n",
    "    WHERE domain = \"nps\" \n",
    "    AND top_level_domain = 'gov'\n",
    "    \"\"\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336d454",
   "metadata": {},
   "source": [
    "### Task 15\n",
    "\n",
    "Close the `SparkSession` and underlying `sparkContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2233037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the notebook's `SparkSession` and `sparkContext`\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
